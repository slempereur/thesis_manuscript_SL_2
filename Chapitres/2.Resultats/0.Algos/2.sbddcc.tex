\providecommand{\main}{../../..}

\documentclass[\main/main.tex]{subfiles}

\begin{document}

\section{Un algorithme qui compense la perte lumineuse en profondeur dans l'échantillon nécessite d'avoir segmenté l'échantillon entier
\label{sbddcc}
}
\sectionmark{SBDDCC}

\subsection{Description des hypothèses sous\hyp{}tendant la logique de l'algorithme}

%%
%
Lors de l'acquisition d'échantillons de \pz{} clarifiés par simple immersion,
il subsiste une diminution de la lumière au sein des tissus.
%
Cette atténuation étant proportionnelle à la quantité de tissus traversée,
on peut supposer qu'elle suit la loi de Beer\hyp{}Lambert décrite en \autoref{sec:beer}.
%
Afin d'améliorer les acquisitions réalisées,
il serait nécessaire d'effectuer une modélisation de l'atténuation lumineuse pour chaque colonne de pixel depuis le plan d'acquisition.
%
Cette solution permettrait d'avoir une compensation idéale de l'atténuation, mais serait chronophage, ce qui n'est pas compatible avec la mise en place d'une procédure de \hcs{}.

%%
%
Cependant, comme expliqué en \autoref{sec:clarification:etapes}, la clarification de tissus permet d'homogéneiser l'indice de réfraction de l'échantillon.
%
En considérant l'indice de réfraction comme constant, l'atténuation par la loi de Beer\hyp{}Lambert ne dépend plus que de la profondeur de tissus pénétrés.

%%
%
En partant de cet a priori, nous avons développé une méthode de compensation de contraste, le \sbddcc (SBDDCC).
%
Cette approche commence par rechercher une métrique à comparer pour chaque couche au sein de l'échantillon.
%
La métrique choisie a été le médian des valeurs de gris de chaque couche.
%
Il aurait été possible de modéliser la décroissance exponentielle au sein des tissus pour obtenir un coefficient d'absorption à appliquer pour l'ensemble des pixels.
%
Cependant, afin de réduire le temps de calcul, il a été choisi de normaliser les valeurs de médians pour les couches profondes.

\subsection{Un algorithme compensant la déperdition lumineuse en fonction de l'épaisseur des tissus traversés}

%
Considérons $I$ l'acquisition à traiter et $S_{\texnormal{larva}}$ la segmentation de la larve entière au sein de cette acquisition (Voir la \autoref{sec:algo:larva} pour une description de l'algorithme permettant la segmentation de la larve entière).

%
Soit $b$ le dernier plan de $I$ dans le sens de l'acquisition.
%
On calcule tout d'abord une somme cumulative de $S_{\texnormal{larva}}$ dans le sens de l'acquisition~\eqref{eq:sbddcc:cum_sum}.
%
On multiplie ensuite cette somme cumulative par $S_{\texnormal{larva}}$ pour obtenir une carte de la quantité de tissus à parcourir depuis le plan d'acquisition~\eqref{eq:sbddcc:carte}.
%
Soit $d$ une quantité de tissus à pénétrer quelconque, $d_b$ la plus grande quantité de tissus à pénétrer.

\begin{equation}
    \label{eq:sbddcc:cum_sum}
    \forall z \in [0;b], 
        C_1(z)=\sum_{n=1}^{z}S_{\textnormal{larva}}(n)
\end{equation}

\begin{equation}
    \label{eq:sbddcc:carte}
    C_2=C_1 * S_{\textnormal{larva}}
\end{equation}
%
On calcule la valeur de gris médian des voxels de I se trouvant pour chaque couche.
%
Appelons $\mu{}(d)$ la valeur médiane de $I$ pour la couche $d$, $\mu_{max}$ la valeur maximale de $\mu{}(d)$ et $d_{max}$ l'indice de $\mu_{max}$ au sein de $\mu{}(d)$.
%
L'amélioration de contraste est réalisé en multipliant chaque valeur de gris au sein des couches $d$ plus grande que $d_{max}$ par $\mu_{max}$ divisé par $\mu_d$~\eqref{eq:sbddcc:correction}.

\begin{equation}
    \label{eq:sbddcc:correction}
    \forall d \in [d_{max};d_b],
        I_{c}(d)=I(d)*\frac{\mu_{\max}}{\mu(d)}
\end{equation}

\end{document}
